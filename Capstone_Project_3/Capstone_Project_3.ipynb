{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ff8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262842bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataframe_clean.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143c88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models.BayesianNetwork import BayesianNetwork\n",
    "from pgmpy.estimators import StructureEstimator, BayesianEstimator, HillClimbSearch, PC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class BayesianModel(BayesianNetwork):\n",
    "\n",
    "    def __init__(self, *, \n",
    "        ebunch=None, \n",
    "        graph_search_algo=HillClimbSearch, \n",
    "        scoring_method='k2score', \n",
    "        start_dag=None, \n",
    "        fixed_edges=set(), \n",
    "        tabu_length=100, \n",
    "        max_indegree=None, \n",
    "        black_list=None, \n",
    "        white_list=None, \n",
    "        epsilon=0.0001, \n",
    "        max_iter=1000000.0, \n",
    "        show_progress=True, \n",
    "\n",
    "        variant='stable',\n",
    "        ci_test='chi_square',\n",
    "        max_cond_vars=5,\n",
    "        return_type='dag',\n",
    "        significance_level=0.01,\n",
    "\n",
    "        estimator=BayesianEstimator, \n",
    "        prior_type='BDeu', \n",
    "        pseudo_counts=[], \n",
    "        equivalent_sample_size=5\n",
    "        ):\n",
    "        \n",
    "        if graph_search_algo == PC and return_type in ['pdag', 'cpdag']:\n",
    "            raise ValueError('BayesianModel currently has no support for PDAG\\'s')\n",
    "        \n",
    "        super().__init__(ebunch)\n",
    "        \n",
    "        self.ebunch = ebunch\n",
    "        self.graph_search_algo = graph_search_algo\n",
    "        self.scoring_method = scoring_method\n",
    "        self.start_dag = start_dag\n",
    "        self.fixed_edges = fixed_edges\n",
    "        self.tabu_length = tabu_length\n",
    "        self.max_indegree = max_indegree\n",
    "        self.black_list = black_list\n",
    "        self.white_list = white_list\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iter = max_iter\n",
    "        self.show_progress = show_progress\n",
    "\n",
    "        self.variant = variant\n",
    "        self.ci_test = ci_test\n",
    "        self.max_cond_vars = max_cond_vars\n",
    "        self.return_type = return_type\n",
    "        self.significance_level = significance_level\n",
    "\n",
    "        self.estimator = estimator\n",
    "        self.prior_type = prior_type\n",
    "        self.pseudo_counts = pseudo_counts\n",
    "        self.equivalent_sample_size = equivalent_sample_size\n",
    "\n",
    "        self._dag = None\n",
    "\n",
    "    def fit(self, X_train, y_train, **fit_params):\n",
    "\n",
    "        data = pd.concat([X_train, y_train], axis=1)\n",
    "        graph_search_est = self.graph_search_algo(data)\n",
    "        if self.graph_search_algo == HillClimbSearch:\n",
    "            parameters = dict(\n",
    "                scoring_method=self.scoring_method,\n",
    "                start_dag=self.start_dag,\n",
    "                fixed_edges=self.fixed_edges,\n",
    "                tabu_length=self.tabu_length,\n",
    "                max_indegree=self.max_indegree,\n",
    "                black_list=self.black_list,\n",
    "                white_list=self.white_list,\n",
    "                epsilon=self.epsilon,\n",
    "                max_iter=self.max_iter,\n",
    "                show_progress=self.show_progress\n",
    "            )\n",
    "        elif self.graph_search_algo == PC:\n",
    "            parameters = dict(\n",
    "                variant=self.variant,\n",
    "                ci_test=self.ci_test,\n",
    "                max_cond_vars=self.max_cond_vars,\n",
    "                return_type=self.return_type,\n",
    "                significance_level=self.significance_level,\n",
    "                show_progress=self.show_progress\n",
    "            )\n",
    "        dag = graph_search_est.estimate(**parameters)\n",
    "\n",
    "        \n",
    "\n",
    "        extra_columns = list(set(data.columns) - set(dag.nodes))\n",
    "        if y_train.name in extra_columns:\n",
    "            raise ValueError('Resulting DAG does not contain target. It cannot be used to make predictions.')\n",
    "        if len(extra_columns) > 0:\n",
    "            data = data.drop(columns=extra_columns)\n",
    "        elif len(extra_columns) < 0:\n",
    "            raise ValueError('Invalid value for extra_columns')\n",
    "        \n",
    "        self._dag = dag\n",
    "        self.ebunch = list(dag.nodes)\n",
    "        super().__init__(dag)\n",
    "        print('Now fitting the graph...')\n",
    "        super().fit(\n",
    "            data, \n",
    "            estimator=self.estimator, \n",
    "            prior_type=self.prior_type, \n",
    "            pseudo_counts=self.pseudo_counts,\n",
    "            equivalent_sample_size=self.equivalent_sample_size,\n",
    "            **fit_params\n",
    "        )\n",
    "        print('Succesfully fitted the graph')\n",
    "\n",
    "        self.X_ = X_train\n",
    "        self.y_ = y_train\n",
    "        self.fit_params_ = fit_params\n",
    "        self.classes_ = LabelEncoder().fit(y_train).classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, stochastic=False, n_jobs=-1):\n",
    "        extra_columns = list(set(X.columns) - set(self.nodes))\n",
    "        if len(extra_columns) > 0:\n",
    "            X = X.drop(columns=extra_columns)\n",
    "        elif len(extra_columns) < 0:\n",
    "            raise ValueError('Invalid value for extra_columns')\n",
    "        y_pred_df = super().predict(X, stochastic, n_jobs)\n",
    "        y_pred = y_pred_df.to_numpy()\n",
    "        self.y_pred_ = y_pred\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        extra_columns = list(set(X.columns) - set(self.nodes))\n",
    "        if len(extra_columns) > 0:\n",
    "            X = X.drop(columns=extra_columns)\n",
    "        elif len(extra_columns) < 0:\n",
    "            raise ValueError('Invalid value for extra_columns')\n",
    "        y_pred_proba_df = super().predict_probability(X)\n",
    "        y_pred_proba = y_pred_proba_df.to_numpy()\n",
    "        self.y_pred_proba_ = y_pred_proba\n",
    "        \n",
    "        return y_pred_proba\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'ebunch':self.ebunch,\n",
    "            'graph_search_algo':self.graph_search_algo,\n",
    "            'scoring_method':self.scoring_method,\n",
    "            'start_dag':self.start_dag,\n",
    "            'fixed_edges':self.fixed_edges,\n",
    "            'tabu_length':self.tabu_length,\n",
    "            'max_indegree':self.max_indegree,\n",
    "            'black_list':self.black_list,\n",
    "            'white_list':self.white_list,\n",
    "            'epsilon':self.epsilon,\n",
    "            'max_iter':self.max_iter,\n",
    "            'show_progress':self.show_progress,\n",
    "            'estimator':self.estimator,\n",
    "            'prior_type':self.prior_type,\n",
    "            'pseudo_counts':self.pseudo_counts,\n",
    "            'equivalent_sample_size':self.equivalent_sample_size\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b56ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def cv_scorer(ml_algo, scoring, *data, model_name=None, algo_params={}, resampler=None, output=True):    \n",
    "    if len(data) == 2:\n",
    "        X, y = tuple(data)\n",
    "        w = None\n",
    "    elif len(data) == 3:\n",
    "        X, y, w = tuple(data)\n",
    "    else:\n",
    "        print('Invalid length for \"data\".')\n",
    "        return\n",
    "    \n",
    "    if resampler != None:\n",
    "        model = Pipeline([('Resampling', resampler()), (model_name, ml_algo(**algo_params))])\n",
    "    else:\n",
    "        model = Pipeline([(model_name, ml_algo(**algo_params))])\n",
    "    \n",
    "    cv_scores = cross_validate(model, X, y, scoring=scoring, fit_params={}, return_estimator=True)\n",
    "    if output == True:\n",
    "        if type(model_name) != type(None):\n",
    "            print(model_name + ' cv_scores:')\n",
    "        else:\n",
    "            print('cv_scores:')\n",
    "        print(cv_scores)\n",
    "        print()\n",
    "    \n",
    "    cv_scores_summary = {}\n",
    "    cv_scores_summary['estimator'] = cv_scores['estimator']\n",
    "    for score in scoring:\n",
    "        scores_ = cv_scores['test_' + score]\n",
    "        mean_ = cv_scores['test_' + score].mean()\n",
    "        std_ = cv_scores['test_' + score].std()\n",
    "        \n",
    "        cv_scores_summary[score]= dict(zip(['scores', 'mean', 'std'], [scores_, mean_, std_]))\n",
    "        \n",
    "        if output == True:\n",
    "            print(score + ' mean: ' + f'{mean_:0.2f}')\n",
    "            print(score + ' std: ' + f'{std_:0.4f}')\n",
    "            print()\n",
    "    if print == True:\n",
    "        print()\n",
    "    \n",
    "    if type(model_name) != type(None):\n",
    "        return {model_name:cv_scores_summary}, cv_scores\n",
    "    else:\n",
    "        return cv_scores_summary, cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d709c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_columns = ['Good_Health', 'Hypertension', 'High_Cholesterol', 'Smoker_Status', 'Age_Cat', 'Diabetes', 'Sodium', 'Heavy_Drinker', 'Heart_Disease', 'SEX', 'Sample_Weights']\n",
    "df = df[included_columns]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed14dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Heart_Disease'] = df['Heart_Disease'].cat.remove_unused_categories()\n",
    "df = df.astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35402d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['Heart_Disease', 'Sample_Weights'])\n",
    "y = df['Heart_Disease']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf826717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_parents_black_list(list_orig, list_no_parents):\n",
    "    '''Given a list of nodes, list_orig, and another list of nodes that should not have parents, \n",
    "    list_no_parents, generates the required black_list for use in graph search algorithms'''\n",
    "\n",
    "    list_start = []\n",
    "    list_end = []\n",
    "    for node_no_parent in list_no_parents:\n",
    "        list_temp_1 = list_orig.copy()\n",
    "        list_temp_1.remove(node_no_parent)\n",
    "        list_temp_2 = [node_no_parent] * len(list_temp_1)\n",
    "        list_start.append(list_temp_1)\n",
    "        list_end.append(list_temp_2)\n",
    "    assert len(list_start) == len(list_end)\n",
    "    \n",
    "    black_list = []\n",
    "    for i in range(len(list_start)):\n",
    "        black_list = black_list + list(zip(list_start[i], list_end[i]))\n",
    "\n",
    "    return black_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def find_threshold(fpr_chosen):\n",
    "    fpr, _, thresholds= roc_curve(y_test, y_proba.iloc[:, 1])\n",
    "    indices = np.where(fpr <= fpr_chosen)[0]\n",
    "    index = indices[-1:-2:-1][0] + 1\n",
    "    return thresholds[index]\n",
    "\n",
    "def predict_with_treshold(proba, threshold):\n",
    "    return (proba >= threshold).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_list = no_parents_black_list(list(pd.concat([X, y], axis=1).columns), ['SEX', 'Age_Cat'])\n",
    "black_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332cce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './cpdag.pkl'\n",
    "if os.path.exists(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        cpdag = pickle.load(f)\n",
    "        print('Loaded cpdag')\n",
    "else:\n",
    "    pc_algo = PC(pd.concat([X_train, y_train], axis=1))\n",
    "    cpdag = pc_algo.estimate(return_type='cpdag')\n",
    "    print('cpdag built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756eb4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nx.draw_shell(cpdag, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './bm_predictions.pkl'\n",
    "if os.path.exists(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        _ = pickle.load(f)\n",
    "        bm = pickle.load(f)\n",
    "        y_test = pickle.load(f)\n",
    "        y_pred = pickle.load(f)\n",
    "        y_proba = pickle.load(f)\n",
    "        print(f'Loaded from {path}.')\n",
    "else:\n",
    "    from pgmpy.estimators import BayesianEstimator\n",
    "\n",
    "    bm = BayesianModel(graph_search_algo=HillClimbSearch, estimator=BayesianEstimator, prior_type='BDeu', equivalent_sample_size=5, black_list=black_list)\n",
    "    bm.fit(X_train, y_train)\n",
    "    y_pred = bm.predict(X_test)\n",
    "    y_proba = bm.predict_proba(X_test)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(4, f)\n",
    "        pickle.dump(bm, f)\n",
    "        pickle.dump(y_test, f)\n",
    "        pickle.dump(y_pred, f)\n",
    "        pickle.dump(y_proba, f)\n",
    "    print('Model trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba[:, 1], pos_label='1')\n",
    "plt.show()\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_proba[:, 1], pos_label='1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af93752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize='true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx.draw_shell(bm, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c090775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import BayesianEstimator\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "roc_auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "scores = cross_validate(BayesianModel(graph_search_algo=HillClimbSearch, estimator=BayesianEstimator, prior_type='BDeu', equivalent_sample_size=5), \n",
    "    X, y, scoring=['recall', 'roc_auc'], cv=StratifiedKFold())\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b07999",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_summary, cv_scores = cv_scorer(BayesianModel, ['recall'], X_train, y_train, model_name='BayesianModel', algo_params=dict(graph_search_algo=HillClimbSearch, \\\n",
    "    graph_search_params={}, estimator=BayesianEstimator, prior_type='BDeu', equivalent_sample_size=5), resampler=None, output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference.CausalInference import CausalInference\n",
    "\n",
    "inference = CausalInference(bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c721030",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_parents = {}\n",
    "for parent in bm.get_parents('Heart_Disease'):\n",
    "    dict_parents[parent]= list(X_train[parent].unique())\n",
    "dict_parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f908c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_dict = {}\n",
    "phi_dict['No intervention'] = inference.query(['Heart_Disease'])\n",
    "for parent, values in dict_parents.items():\n",
    "    for value in values:\n",
    "        name = f'{parent}_{value}'\n",
    "        phi_dict[name] = inference.query(['Heart_Disease'], do={parent:value})\n",
    "phi_dict['Hypertension_2.0, High_Cholesterol_2.0'] = inference.query(['Heart_Disease'], do={'Hypertension':'2', 'High_Cholesterol':'2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f1c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_factor_to_series(factor, name=None):\n",
    "    variables = factor.scope()\n",
    "    n_states = 1\n",
    "    for cardinality in factor.get_cardinality(variables).values():\n",
    "        n_states = n_states * cardinality\n",
    "    state_indices = list(range(n_states))\n",
    "    assignments = factor.assignment(state_indices)\n",
    "    state_names = []\n",
    "    for assignment in assignments:\n",
    "        state_name = ''\n",
    "        for tuple_ in assignment:\n",
    "            state_name = state_name +  '__' + tuple_[0] + '_' + tuple_[1]\n",
    "        state_names.append(state_name)\n",
    "    states_dicts = [{state[0]:state[1] for state in assignment} for assignment in assignments]\n",
    "    values = [round(factor.get_value(**dict_), 4) for dict_ in states_dicts]\n",
    "\n",
    "    return pd.Series(values, index=state_names, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_list = []\n",
    "for name, factor in phi_dict.items():\n",
    "    series_list.append(discrete_factor_to_series(factor, name))\n",
    "\n",
    "infer_df = pd.DataFrame()\n",
    "columns = []\n",
    "for series in series_list:\n",
    "    columns.append(series.name)\n",
    "    infer_df = pd.concat([infer_df, series], axis=1)\n",
    "infer_df.columns = columns\n",
    "infer_df = infer_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b7023e0",
   "metadata": {},
   "source": [
    "**Testing Active Trail Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model = BayesianNetwork([('Difficulty', 'Grade'), ('Intelligence', 'Grade'),('Intelligence', 'SAT'), ('Grade', 'Letter')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.get_random_cpds(n_states = {'Difficulty':2, 'Intelligence':2, 'Grade':5, 'SAT':2, 'Letter':2}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241bc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(BayesianNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(toy_model, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in toy_model.nodes:\n",
    "    print(toy_model.active_trail_nodes(node, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06afe607",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy = pd.DataFrame(index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57afb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.predict_probability(df_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.active_trail_nodes(['SAT'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "33bf6ce7a567026968bb969a586c378ea41bdd87d8aea5d31ce5ed3fdb904eb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
