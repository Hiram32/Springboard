{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0ff8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262842bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataframe_clean.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece076ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Good_Health</th>\n",
       "      <th>Health_Insurance</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>High_Cholesterol</th>\n",
       "      <th>Asthma_Status</th>\n",
       "      <th>Arthritis</th>\n",
       "      <th>Race</th>\n",
       "      <th>Age_Cat</th>\n",
       "      <th>BMI_Cat</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>...</th>\n",
       "      <th>Pneumonia_Vaccine</th>\n",
       "      <th>HIV</th>\n",
       "      <th>Bronchitis</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Kidney_Disease</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>SEX</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Sodium</th>\n",
       "      <th>Heart_Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441451</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441452</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441453</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441454</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441455</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437514 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Good_Health Health_Insurance Hypertension High_Cholesterol  \\\n",
       "0              2.0              1.0          1.0              1.0   \n",
       "1              1.0              2.0          2.0              2.0   \n",
       "3              2.0              1.0          1.0              1.0   \n",
       "4              2.0              1.0          2.0              2.0   \n",
       "5              1.0              9.0          1.0              2.0   \n",
       "...            ...              ...          ...              ...   \n",
       "441451         2.0              9.0          1.0              1.0   \n",
       "441452         1.0              1.0          2.0              2.0   \n",
       "441453         2.0              9.0          1.0              1.0   \n",
       "441454         1.0              1.0          1.0              2.0   \n",
       "441455         1.0              1.0          1.0              1.0   \n",
       "\n",
       "       Asthma_Status Arthritis Race Age_Cat BMI_Cat Education_Level  ...  \\\n",
       "0                1.0       1.0  1.0     5.0     4.0             2.0  ...   \n",
       "1                3.0       2.0  1.0     4.0     3.0             4.0  ...   \n",
       "3                3.0       1.0  1.0     5.0     3.0             2.0  ...   \n",
       "4                3.0       1.0  1.0     5.0     2.0             3.0  ...   \n",
       "5                3.0       1.0  1.0     6.0     3.0             1.0  ...   \n",
       "...              ...       ...  ...     ...     ...             ...  ...   \n",
       "441451           3.0       1.0  5.0     6.0     1.0             1.0  ...   \n",
       "441452           3.0       2.0  5.0     2.0     3.0             3.0  ...   \n",
       "441453           3.0       2.0  5.0     6.0     4.0             2.0  ...   \n",
       "441454           3.0       2.0  5.0     4.0     2.0             3.0  ...   \n",
       "441455           3.0       2.0  5.0     5.0     2.0             4.0  ...   \n",
       "\n",
       "       Pneumonia_Vaccine    HIV Bronchitis Depression Kidney_Disease Diabetes  \\\n",
       "0                   65.0    1.0        1.0        1.0            2.0      2.0   \n",
       "1                   65.0    2.0        2.0        2.0            2.0      2.0   \n",
       "3                   65.0    9.0        2.0        1.0            2.0      2.0   \n",
       "4                   65.0    1.0        2.0        2.0            2.0      2.0   \n",
       "5                    1.0    2.0        2.0        2.0            2.0      2.0   \n",
       "...                  ...    ...        ...        ...            ...      ...   \n",
       "441451               2.0    2.0        2.0        2.0            2.0      1.0   \n",
       "441452              65.0    1.0        2.0        2.0            2.0      2.0   \n",
       "441453               9.0  999.0        2.0        2.0            2.0      2.0   \n",
       "441454              65.0    2.0        2.0        2.0            2.0      2.0   \n",
       "441455              65.0    1.0        1.0        2.0            2.0      1.0   \n",
       "\n",
       "        SEX Marital_Status Sodium Heart_Disease  \n",
       "0       2.0            1.0  999.0           0.0  \n",
       "1       2.0            2.0    2.0           0.0  \n",
       "3       2.0            1.0    2.0           0.0  \n",
       "4       2.0            1.0    2.0           0.0  \n",
       "5       2.0            3.0    2.0           0.0  \n",
       "...     ...            ...    ...           ...  \n",
       "441451  2.0            3.0    1.0           0.0  \n",
       "441452  2.0            1.0    2.0           0.0  \n",
       "441453  2.0            1.0  999.0           0.0  \n",
       "441454  1.0            5.0    1.0           0.0  \n",
       "441455  2.0            1.0    1.0           1.0  \n",
       "\n",
       "[437514 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns='Sample_Weights')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a536c6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Good_Health', 'Health_Insurance', 'Hypertension', 'High_Cholesterol',\n",
       "       'Asthma_Status', 'Arthritis', 'Race', 'Age_Cat', 'BMI_Cat',\n",
       "       'Education_Level', 'Income_Level', 'Smoker_Status', 'Heavy_Drinker',\n",
       "       'Physical_Activity', 'Seatbelt', 'Flu_Shot', 'Pneumonia_Vaccine', 'HIV',\n",
       "       'Bronchitis', 'Depression', 'Kidney_Disease', 'Diabetes', 'SEX',\n",
       "       'Marital_Status', 'Sodium', 'Heart_Disease'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "143c88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models.BayesianNetwork import BayesianNetwork\n",
    "from pgmpy.estimators import StructureEstimator, HillClimbSearch\n",
    "\n",
    "class BayesianModel(BayesianNetwork):\n",
    "    cls = {'graph_search_algo':None, \\\n",
    "           'graph_search_params':{}, \\\n",
    "           'X':pd.DataFrame(), \\\n",
    "           'y':pd.Series(dtype=float), \\\n",
    "           'dag':None}\n",
    "\n",
    "    def __init__(self, *, ebunch=None, graph_search_algo=HillClimbSearch, graph_search_params={}, **params):\n",
    "        super().__init__(ebunch)\n",
    "        \n",
    "        if ebunch == None:\n",
    "            self.graph_search_algo = graph_search_algo\n",
    "            self.graph_search_params = graph_search_params\n",
    "        else:\n",
    "            self.graph_search_algo = None\n",
    "            self.graph_search_params = None\n",
    "\n",
    "        self.params = params\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        skip_search_graph = True\n",
    "        skip_search_graph = BayesianModel.cls['graph_search_algo'] == self.graph_search_algo\n",
    "        if skip_search_graph:\n",
    "            skip_search_graph = BayesianModel.cls['graph_search_params'] == self.graph_search_params\n",
    "        if skip_search_graph:\n",
    "            if BayesianModel.cls['X'].shape == X.shape:\n",
    "                if (BayesianModel.cls['X'].columns == X.columns).all() and (BayesianModel.cls['X'].index == X.index).all():\n",
    "                    skip_search_graph = (BayesianModel.cls['X'] == X).all().all()\n",
    "                else:\n",
    "                    skip_search_graph = False\n",
    "            else:\n",
    "                skip_search_graph = False\n",
    "        if skip_search_graph:\n",
    "            skip_search_graph = (BayesianModel.cls['y'] == y).all()\n",
    "        \n",
    "        data = pd.concat([X, y], axis=1)\n",
    "        if not skip_search_graph:\n",
    "            graph_search_est = self.graph_search_algo(data)\n",
    "            dag = graph_search_est.estimate(**self.graph_search_params)\n",
    "            super().__init__(dag)\n",
    "\n",
    "            BayesianModel.cls['graph_search_algo'] = self.graph_search_algo\n",
    "            BayesianModel.cls['graph_search_params'] = self.graph_search_params\n",
    "            BayesianModel.cls['X'] = X\n",
    "            BayesianModel.cls['y'] = y\n",
    "            BayesianModel.cls['dag'] = dag\n",
    "        else:\n",
    "            print('Graph Searching has been skipped')\n",
    "            super().__init__(BayesianModel.cls['dag'])\n",
    "\n",
    "        if len(fit_params) != 0:\n",
    "            params = {**self.params, **fit_params}\n",
    "        else:\n",
    "            params = self.params\n",
    "        \n",
    "        print('Now fitting the graph...')\n",
    "        super().fit(data, **params)\n",
    "        print('Succesfully fitted the graph')\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return {'graph_search_algo':self.graph_search_algo, 'graph_search_params':self.graph_search_params, **self.params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "840a5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8b56ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_scorer(ml_algo, scoring, *data, model_name=None, algo_params={}, resampler=None, output=True):    \n",
    "    if len(data) == 2:\n",
    "        X, y = tuple(data)\n",
    "        w = None\n",
    "    elif len(data) == 3:\n",
    "        X, y, w = tuple(data)\n",
    "    else:\n",
    "        print('Invalid length for \"data\".')\n",
    "        return\n",
    "    \n",
    "    if resampler != None:\n",
    "        model = Pipeline([('Resampling', resampler()), (model_name, ml_algo(**algo_params))])\n",
    "    else:\n",
    "        model = Pipeline([(model_name, ml_algo(**algo_params))])\n",
    "    \n",
    "    cv_scores = cross_validate(model, X, y, scoring=scoring, fit_params={}, return_estimator=True)\n",
    "    if output == True:\n",
    "        if type(model_name) != type(None):\n",
    "            print(model_name + ' cv_scores:')\n",
    "        else:\n",
    "            print('cv_scores:')\n",
    "        print(cv_scores)\n",
    "        print()\n",
    "    \n",
    "    cv_scores_summary = {}\n",
    "    cv_scores_summary['estimator'] = cv_scores['estimator']\n",
    "    for score in scoring:\n",
    "        scores_ = cv_scores['test_' + score]\n",
    "        mean_ = cv_scores['test_' + score].mean()\n",
    "        std_ = cv_scores['test_' + score].std()\n",
    "        \n",
    "        cv_scores_summary[score]= dict(zip(['scores', 'mean', 'std'], [scores_, mean_, std_]))\n",
    "        \n",
    "        if output == True:\n",
    "            print(score + ' mean: ' + f'{mean_:0.2f}')\n",
    "            print(score + ' std: ' + f'{std_:0.4f}')\n",
    "            print()\n",
    "    if print == True:\n",
    "        print()\n",
    "    \n",
    "    if type(model_name) != type(None):\n",
    "        return {model_name:cv_scores_summary}, cv_scores\n",
    "    else:\n",
    "        return cv_scores_summary, cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5698420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import BayesianEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b34d453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c735f61592a4db2ba2d3df9226c9b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fitting the graph...\n",
      "Succesfully fitted the graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de573563513140c8a476a556fe2b7779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/queues.py\", line 153, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 271, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 264, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_memmapping_reducer.py\", line 442, in __call__\n",
      "    for dumped_filename in dump(a, filename):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 482, in dump\n",
      "    NumpyPickler(f, protocol=protocol).dump(value)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/pickle.py\", line 487, in dump\n",
      "    self.save(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 281, in save\n",
      "    wrapper.write_array(obj, self)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 104, in write_array\n",
      "    pickler.file_handle.write(chunk.tobytes('C'))\n",
      "OSError: [Errno 28] No space left on device\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/pgmpy/models/BayesianNetwork.py\", line 734, in predict\n",
      "    pred_values = Parallel(n_jobs=n_jobs)(\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "_pickle.PicklingError: Could not pickle the task to send it to the workers.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce454b460d44f158429f94700784492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fitting the graph...\n",
      "Succesfully fitted the graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19fc9c7fe04429b8557b5a29673f32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80078 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/queues.py\", line 153, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 271, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 264, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_memmapping_reducer.py\", line 442, in __call__\n",
      "    for dumped_filename in dump(a, filename):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 482, in dump\n",
      "    NumpyPickler(f, protocol=protocol).dump(value)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/pickle.py\", line 487, in dump\n",
      "    self.save(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 281, in save\n",
      "    wrapper.write_array(obj, self)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 104, in write_array\n",
      "    pickler.file_handle.write(chunk.tobytes('C'))\n",
      "OSError: [Errno 28] No space left on device\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/pgmpy/models/BayesianNetwork.py\", line 734, in predict\n",
      "    pred_values = Parallel(n_jobs=n_jobs)(\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "_pickle.PicklingError: Could not pickle the task to send it to the workers.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb6b659964c47259d20ff3f9780ef43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fitting the graph...\n",
      "Succesfully fitted the graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38413854a7e54493acce9bf10826bcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/queues.py\", line 153, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 271, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 264, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_memmapping_reducer.py\", line 442, in __call__\n",
      "    for dumped_filename in dump(a, filename):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 482, in dump\n",
      "    NumpyPickler(f, protocol=protocol).dump(value)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/pickle.py\", line 487, in dump\n",
      "    self.save(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 281, in save\n",
      "    wrapper.write_array(obj, self)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 104, in write_array\n",
      "    pickler.file_handle.write(chunk.tobytes('C'))\n",
      "OSError: [Errno 28] No space left on device\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/pgmpy/models/BayesianNetwork.py\", line 734, in predict\n",
      "    pred_values = Parallel(n_jobs=n_jobs)(\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "_pickle.PicklingError: Could not pickle the task to send it to the workers.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e87af738a94041a3cabd18d40768fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fitting the graph...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cross_validate(BayesianModel(graph_search_algo\u001b[39m=\u001b[39;49mHillClimbSearch, graph_search_params\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(max_iter\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m), estimator\u001b[39m=\u001b[39;49mBayesianEstimator, prior_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mBDeu\u001b[39;49m\u001b[39m'\u001b[39;49m, equivalent_sample_size\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m), \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     X, y, scoring\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mrecall\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "\u001b[1;32m/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb Cell 9\u001b[0m in \u001b[0;36mBayesianModel.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb#X62sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb#X62sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNow fitting the graph...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb#X62sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(data, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb#X62sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSuccesfully fitted the graph\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/pgmpy/models/BayesianNetwork.py:586\u001b[0m, in \u001b[0;36mBayesianNetwork.fit\u001b[0;34m(self, data, estimator, state_names, complete_samples_only, n_jobs, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEstimator object should be a valid pgmpy estimator.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    580\u001b[0m _estimator \u001b[39m=\u001b[39m estimator(\n\u001b[1;32m    581\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    582\u001b[0m     data,\n\u001b[1;32m    583\u001b[0m     state_names\u001b[39m=\u001b[39mstate_names,\n\u001b[1;32m    584\u001b[0m     complete_samples_only\u001b[39m=\u001b[39mcomplete_samples_only,\n\u001b[1;32m    585\u001b[0m )\n\u001b[0;32m--> 586\u001b[0m cpds_list \u001b[39m=\u001b[39m _estimator\u001b[39m.\u001b[39;49mget_parameters(n_jobs\u001b[39m=\u001b[39;49mn_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    587\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_cpds(\u001b[39m*\u001b[39mcpds_list)\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/pgmpy/estimators/BayesianEstimator.py:108\u001b[0m, in \u001b[0;36mBayesianEstimator.get_parameters\u001b[0;34m(self, prior_type, equivalent_sample_size, pseudo_counts, n_jobs, weighted)\u001b[0m\n\u001b[1;32m     99\u001b[0m     cpd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimate_cpd(\n\u001b[1;32m    100\u001b[0m         node,\n\u001b[1;32m    101\u001b[0m         prior_type\u001b[39m=\u001b[39mprior_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m         weighted\u001b[39m=\u001b[39mweighted,\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m cpd\n\u001b[0;32m--> 108\u001b[0m parameters \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49mn_jobs, prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m)(\n\u001b[1;32m    109\u001b[0m     delayed(_get_node_param)(node) \u001b[39mfor\u001b[39;49;00m node \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mnodes()\n\u001b[1;32m    110\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[39mreturn\u001b[39;00m parameters\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1057\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    936\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cross_validate(BayesianModel(graph_search_algo=HillClimbSearch, graph_search_params=dict(max_iter=20), estimator=BayesianEstimator, prior_type='BDeu', equivalent_sample_size=5), \\\n",
    "    X, y, scoring=['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c1bbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns='Heart_Disease')\n",
    "y = df['Heart_Disease']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48ddec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import BayesianEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c090775c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_params() got an unexpected keyword argument 'deep'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py:822\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 822\u001b[0m     tasks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ready_batches\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    823\u001b[0m \u001b[39mexcept\u001b[39;00m queue\u001b[39m.\u001b[39mEmpty:\n\u001b[1;32m    824\u001b[0m     \u001b[39m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[1;32m    825\u001b[0m     \u001b[39m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[39m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[1;32m    829\u001b[0m     \u001b[39m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 168\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cross_validate(BayesianModel(graph_search_algo\u001b[39m=\u001b[39;49mHillClimbSearch, graph_search_params\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mtabu_length\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m200\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmax_iter\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m20\u001b[39;49m}, estimator\u001b[39m=\u001b[39;49mBayesianEstimator, prior_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mBDeu\u001b[39;49m\u001b[39m'\u001b[39;49m, equivalent_sample_size\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m), \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                X, y, scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrecall\u001b[39;49m\u001b[39m'\u001b[39;49m, )\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py:833\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    830\u001b[0m n_jobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_effective_n_jobs\n\u001b[1;32m    831\u001b[0m big_batch_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m n_jobs\n\u001b[0;32m--> 833\u001b[0m islice \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(iterator, big_batch_size))\n\u001b[1;32m    834\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(islice) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    835\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:268\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m    266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m--> 268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/base.py:85\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mCannot clone object \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m (type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m): \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mit does not seem to be a scikit-learn \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mestimator as it does not implement a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m'\u001b[39m\u001b[39m method.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mrepr\u001b[39m(estimator), \u001b[39mtype\u001b[39m(estimator))\n\u001b[1;32m     82\u001b[0m             )\n\u001b[1;32m     84\u001b[0m klass \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[0;32m---> 85\u001b[0m new_object_params \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m new_object_params\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     87\u001b[0m     new_object_params[name] \u001b[39m=\u001b[39m clone(param, safe\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_params() got an unexpected keyword argument 'deep'"
     ]
    }
   ],
   "source": [
    "cross_validate(BayesianModel(graph_search_algo=HillClimbSearch, graph_search_params={'tabu_length':200, 'max_iter':20}, estimator=BayesianEstimator, prior_type='BDeu', equivalent_sample_size=5), \\\n",
    "               X, y, scoring='recall', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60b07999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a6902e1c024279ae2f17844943c76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fitting the graph...\n",
      "Succesfully fitted the graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2123c9ddbbc445f09499e8fc653c1edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/queues.py\", line 153, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 271, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 264, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_memmapping_reducer.py\", line 442, in __call__\n",
      "    for dumped_filename in dump(a, filename):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 482, in dump\n",
      "    NumpyPickler(f, protocol=protocol).dump(value)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/pickle.py\", line 487, in dump\n",
      "    self.save(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 281, in save\n",
      "    wrapper.write_array(obj, self)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 104, in write_array\n",
      "    pickler.file_handle.write(chunk.tobytes('C'))\n",
      "OSError: [Errno 28] No space left on device\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/pipeline.py\", line 458, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/pgmpy/models/BayesianNetwork.py\", line 734, in predict\n",
      "    pred_values = Parallel(n_jobs=n_jobs)(\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "_pickle.PicklingError: Could not pickle the task to send it to the workers.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63caca88afa84b359ef61d2a9ebe2c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fitting the graph...\n",
      "Succesfully fitted the graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f702be83da43484083ec1de52d4737bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80078 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/queues.py\", line 153, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 271, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 264, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_memmapping_reducer.py\", line 442, in __call__\n",
      "    for dumped_filename in dump(a, filename):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 482, in dump\n",
      "    NumpyPickler(f, protocol=protocol).dump(value)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/pickle.py\", line 487, in dump\n",
      "    self.save(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 281, in save\n",
      "    wrapper.write_array(obj, self)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 104, in write_array\n",
      "    pickler.file_handle.write(chunk.tobytes('C'))\n",
      "OSError: [Errno 28] No space left on device\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/pipeline.py\", line 458, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/pgmpy/models/BayesianNetwork.py\", line 734, in predict\n",
      "    pred_values = Parallel(n_jobs=n_jobs)(\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "_pickle.PicklingError: Could not pickle the task to send it to the workers.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1615751801497186af1027a6ab5a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fitting the graph...\n",
      "Succesfully fitted the graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4a8c99b3754a0c842c9e62b86158ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/queues.py\", line 153, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 271, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 264, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_memmapping_reducer.py\", line 442, in __call__\n",
      "    for dumped_filename in dump(a, filename):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 482, in dump\n",
      "    NumpyPickler(f, protocol=protocol).dump(value)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/pickle.py\", line 487, in dump\n",
      "    self.save(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 281, in save\n",
      "    wrapper.write_array(obj, self)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 104, in write_array\n",
      "    pickler.file_handle.write(chunk.tobytes('C'))\n",
      "OSError: [Errno 28] No space left on device\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/pipeline.py\", line 458, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/pgmpy/models/BayesianNetwork.py\", line 734, in predict\n",
      "    pred_values = Parallel(n_jobs=n_jobs)(\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "_pickle.PicklingError: Could not pickle the task to send it to the workers.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2820a6f783a84e6284bf174a963d46fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fitting the graph...\n",
      "Succesfully fitted the graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610a5a2454134740b980457b5de87a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/queues.py\", line 153, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 271, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 264, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_memmapping_reducer.py\", line 442, in __call__\n",
      "    for dumped_filename in dump(a, filename):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 482, in dump\n",
      "    NumpyPickler(f, protocol=protocol).dump(value)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/pickle.py\", line 487, in dump\n",
      "    self.save(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 281, in save\n",
      "    wrapper.write_array(obj, self)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 104, in write_array\n",
      "    pickler.file_handle.write(chunk.tobytes('C'))\n",
      "OSError: [Errno 28] No space left on device\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/pipeline.py\", line 458, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/pgmpy/models/BayesianNetwork.py\", line 734, in predict\n",
      "    pred_values = Parallel(n_jobs=n_jobs)(\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "_pickle.PicklingError: Could not pickle the task to send it to the workers.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ed06875ae745bab93af48bde7dffad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fitting the graph...\n",
      "Succesfully fitted the graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a055abadcc549aa917ccb21c681f534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesianModel cv_scores:\n",
      "{'fit_time': array([45.60067487, 47.88123989, 46.099684  , 46.32621431, 46.69293022]), 'score_time': array([39.59282041, 42.04120445, 39.19882369, 40.80674863, 39.95348144]), 'estimator': [Pipeline(steps=[('BayesianModel',\n",
      "                 <__main__.BayesianModel object at 0x7fb6795260a0>)]), Pipeline(steps=[('BayesianModel',\n",
      "                 <__main__.BayesianModel object at 0x7fb678e5b8b0>)]), Pipeline(steps=[('BayesianModel',\n",
      "                 <__main__.BayesianModel object at 0x7fb678fa8b50>)]), Pipeline(steps=[('BayesianModel',\n",
      "                 <__main__.BayesianModel object at 0x7fb6794eb8e0>)]), Pipeline(steps=[('BayesianModel',\n",
      "                 <__main__.BayesianModel object at 0x7fb67939afa0>)])], 'test_recall': array([nan, nan, nan, nan, nan])}\n",
      "\n",
      "recall mean: nan\n",
      "recall std: nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/queues.py\", line 153, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 271, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 264, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_memmapping_reducer.py\", line 442, in __call__\n",
      "    for dumped_filename in dump(a, filename):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 482, in dump\n",
      "    NumpyPickler(f, protocol=protocol).dump(value)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/pickle.py\", line 487, in dump\n",
      "    self.save(obj)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 281, in save\n",
      "    wrapper.write_array(obj, self)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/numpy_pickle.py\", line 104, in write_array\n",
      "    pickler.file_handle.write(chunk.tobytes('C'))\n",
      "OSError: [Errno 28] No space left on device\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/sklearn/pipeline.py\", line 458, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/pgmpy/models/BayesianNetwork.py\", line 734, in predict\n",
      "    pred_values = Parallel(n_jobs=n_jobs)(\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"/home/hiram/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "_pickle.PicklingError: Could not pickle the task to send it to the workers.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cv_scores_summary, cv_scores = cv_scorer(BayesianModel, ['recall'], X, y, model_name='BayesianModel', algo_params=dict(graph_search_algo=HillClimbSearch, \\\n",
    "    graph_search_params={'tabu_length':200, 'max_iter':20}, estimator=BayesianEstimator, prior_type='BDeu', equivalent_sample_size=5), resampler=None, output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8cda307",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest model trained.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     best_model \u001b[39m=\u001b[39m BayesianNetwork\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mbest_model.bif\u001b[39;49m\u001b[39m'\u001b[39;49m, filetype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbif\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hiram/Capstone_Project_3/Capstone_Project_3.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest model loaded.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/pgmpy/models/BayesianNetwork.py:1397\u001b[0m, in \u001b[0;36mBayesianNetwork.load\u001b[0;34m(filename, filetype)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[39mif\u001b[39;00m filetype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbif\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1395\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpgmpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreadwrite\u001b[39;00m \u001b[39mimport\u001b[39;00m BIFReader\n\u001b[0;32m-> 1397\u001b[0m     reader \u001b[39m=\u001b[39m BIFReader(path\u001b[39m=\u001b[39;49mfilename)\n\u001b[1;32m   1398\u001b[0m     \u001b[39mreturn\u001b[39;00m reader\u001b[39m.\u001b[39mget_model()\n\u001b[1;32m   1400\u001b[0m \u001b[39melif\u001b[39;00m filetype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muai\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/pgmpy/readwrite/BIF.py:96\u001b[0m, in \u001b[0;36mBIFReader.__init__\u001b[0;34m(self, path, string, include_properties, n_jobs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariable_properties \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_property()\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariable_parents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_parents()\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariable_cpds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_values()\n\u001b[1;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariable_edges \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_edges()\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/pgmpy/readwrite/BIF.py:319\u001b[0m, in \u001b[0;36mBIFReader.get_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_values\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    300\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39m    Returns the CPD of the variables present in the network\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39m                        [0.4, 0.95]])}\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m     cpd_values \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[1;32m    320\u001b[0m         delayed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_values_from_block)(block)\n\u001b[1;32m    321\u001b[0m         \u001b[39mfor\u001b[39;49;00m block \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobability_block()\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    324\u001b[0m     variable_cpds \u001b[39m=\u001b[39m {}\n\u001b[1;32m    325\u001b[0m     \u001b[39mfor\u001b[39;00m var_name, arr \u001b[39min\u001b[39;00m cpd_values:\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1057\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    936\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/hiram/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pgmpy.models.BayesianNetwork import BayesianNetwork\n",
    "\n",
    "path = 'best_model.bif'\n",
    "path_exists = os.path.exists(path)\n",
    "\n",
    "if not path_exists:\n",
    "    from pgmpy.estimators import HillClimbSearch, BayesianEstimator\n",
    "    \n",
    "    data_train = pd.concat([X_train, y_train], axis=1)\n",
    "    est = HillClimbSearch(data_train)\n",
    "    best_model = BayesianNetwork(est.estimate())\n",
    "    best_model.fit(data_train, estimator=BayesianEstimator, prior_type='BDeu', equivalent_sample_size=5)\n",
    "    del(data_train)\n",
    "    best_model.save('best_model.bif', filetype='bif')\n",
    "    print('Best model trained.')\n",
    "else:\n",
    "    \n",
    "    best_model = BayesianNetwork.load('best_model.bif', filetype='bif')\n",
    "    print('Best model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx.draw_kamada_kawai(bm, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'y_predictions.pkl'\n",
    "path_exists = os.path.exists(path)\n",
    "\n",
    "if not path_exists:\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_probability(X_test)\n",
    "\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(2, f)\n",
    "        pickle.dump(y_pred, f)\n",
    "        pickle.dump(y_pred_proba, f)\n",
    "\n",
    "else:\n",
    "    with open(path, 'rb') as f:\n",
    "        _ = pickle.load(f)\n",
    "        y_pred = pickle.load(f)\n",
    "        y_pred_proba = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48853ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize='true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d702bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba.iloc[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def find_threshold(fpr_chosen):\n",
    "    fpr, _, thresholds= roc_curve(y_test, y_pred_proba.iloc[:, 1])\n",
    "    indices = np.where(fpr <= fpr_chosen)[0]\n",
    "    index = indices[-1:-2:-1][0] + 1\n",
    "    return thresholds[index]\n",
    "\n",
    "def predict_with_treshold(proba, threshold):\n",
    "    return (proba >= threshold).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = predict_with_treshold(y_pred_proba.iloc[:, 1], find_threshold(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d5d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67513c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_2, normalize='true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86423602",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Node', 'Parents', 'InDegree')\n",
    "for node in best_model.nodes:\n",
    "    parents = best_model.get_parents(node)\n",
    "    print(node, parents, len(parents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference.CausalInference import CausalInference\n",
    "\n",
    "inference = CausalInference(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3dab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = inference.query(['Heart_Disease'], do={'Hypertension':2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19124a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.estimate_ate('Hypertension', 'Heart_Disease', data_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b7023e0",
   "metadata": {},
   "source": [
    "**Testing Active Trail Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model = BayesianNetwork([('Difficulty', 'Grade'), ('Intelligence', 'Grade'),('Intelligence', 'SAT'), ('Grade', 'Letter')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.get_random_cpds(n_states = {'Difficulty':2, 'Intelligence':2, 'Grade':5, 'SAT':2, 'Letter':2}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241bc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(BayesianNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(toy_model, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in toy_model.nodes:\n",
    "    print(toy_model.active_trail_nodes(node, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06afe607",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy = pd.DataFrame(index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57afb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.predict_probability(df_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.active_trail_nodes(['SAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference.CausalInference import CausalInference\n",
    "inference = CausalInference(toy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('Grade:', i)\n",
    "    print(inference.query(['Letter'], do={'Grade':i}), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22efd236",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.get_all_backdoor_adjustment_sets('Intelligence', 'Letter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "33bf6ce7a567026968bb969a586c378ea41bdd87d8aea5d31ce5ed3fdb904eb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
