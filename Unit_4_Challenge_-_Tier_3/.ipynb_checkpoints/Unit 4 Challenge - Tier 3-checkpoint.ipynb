{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tia3MP1SJpgj"
   },
   "source": [
    "# Springboard Data Science Career Track Unit 4 Challenge - Tier 3 Complete\n",
    "\n",
    "## Objectives\n",
    "Hey! Great job getting through those challenging DataCamp courses. You're learning a lot in a short span of time. \n",
    "\n",
    "In this notebook, you're going to apply the skills you've been learning, bridging the gap between the controlled environment of DataCamp and the *slightly* messier work that data scientists do with actual datasets!\n",
    "\n",
    "Here’s the mystery we’re going to solve: ***which boroughs of London have seen the greatest increase in housing prices, on average, over the last two decades?***\n",
    "\n",
    "\n",
    "A borough is just a fancy word for district. You may be familiar with the five boroughs of New York… well, there are 32 boroughs within Greater London [(here's some info for the curious)](https://en.wikipedia.org/wiki/London_boroughs). Some of them are more desirable areas to live in, and the data will reflect that with a greater rise in housing prices.\n",
    "\n",
    "***This is the Tier 3 notebook, which means it's not filled in at all: we'll just give you the skeleton of a project, the brief and the data. It's up to you to play around with it and see what you can find out! Good luck! If you struggle, feel free to look at easier tiers for help; but try to dip in and out of them, as the more independent work you do, the better it is for your learning!***\n",
    "\n",
    "This challenge will make use of only what you learned in the following DataCamp courses: \n",
    "- Prework courses (Introduction to Python for Data Science, Intermediate Python for Data Science)\n",
    "- Data Types for Data Science\n",
    "- Python Data Science Toolbox (Part One) \n",
    "- pandas Foundations\n",
    "- Manipulating DataFrames with pandas\n",
    "- Merging DataFrames with pandas\n",
    "\n",
    "Of the tools, techniques and concepts in the above DataCamp courses, this challenge should require the application of the following: \n",
    "- **pandas**\n",
    "    - **data ingestion and inspection** (pandas Foundations, Module One) \n",
    "    - **exploratory data analysis** (pandas Foundations, Module Two)\n",
    "    - **tidying and cleaning** (Manipulating DataFrames with pandas, Module Three) \n",
    "    - **transforming DataFrames** (Manipulating DataFrames with pandas, Module One)\n",
    "    - **subsetting DataFrames with lists** (Manipulating DataFrames with pandas, Module One) \n",
    "    - **filtering DataFrames** (Manipulating DataFrames with pandas, Module One) \n",
    "    - **grouping data** (Manipulating DataFrames with pandas, Module Four) \n",
    "    - **melting data** (Manipulating DataFrames with pandas, Module Three) \n",
    "    - **advanced indexing** (Manipulating DataFrames with pandas, Module Four) \n",
    "- **matplotlib** (Intermediate Python for Data Science, Module One)\n",
    "- **fundamental data types** (Data Types for Data Science, Module One) \n",
    "- **dictionaries** (Intermediate Python for Data Science, Module Two)\n",
    "- **handling dates and times** (Data Types for Data Science, Module Four)\n",
    "- **function definition** (Python Data Science Toolbox - Part One, Module One)\n",
    "- **default arguments, variable length, and scope** (Python Data Science Toolbox - Part One, Module Two) \n",
    "- **lambda functions and error handling** (Python Data Science Toolbox - Part One, Module Four) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ipgd2nV8Jpgl"
   },
   "source": [
    "## The Data Science Pipeline\n",
    "\n",
    "This is Tier Three, so we'll get you started. But after that, it's all in your hands! When you feel done with your investigations, look back over what you've accomplished, and prepare a quick presentation of your findings for the next mentor meeting. \n",
    "\n",
    "Data Science is magical. In this case study, you'll get to apply some complex machine learning algorithms. But as  [David Spiegelhalter](https://www.youtube.com/watch?v=oUs1uvsz0Ok) reminds us, there is no substitute for simply **taking a really, really good look at the data.** Sometimes, this is all we need to answer our question.\n",
    "\n",
    "Data Science projects generally adhere to the four stages of Data Science Pipeline:\n",
    "1. Sourcing and loading \n",
    "2. Cleaning, transforming, and visualizing \n",
    "3. Modeling \n",
    "4. Evaluating and concluding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zswDqbefJpgm"
   },
   "source": [
    "### 1. Sourcing and Loading \n",
    "\n",
    "Any Data Science project kicks off by importing  ***pandas***. The documentation of this wonderful library can be found [here](https://pandas.pydata.org/). As you've seen, pandas is conveniently connected to the [Numpy](http://www.numpy.org/) and [Matplotlib](https://matplotlib.org/) libraries. \n",
    "\n",
    "***Hint:*** This part of the data science pipeline will test those skills you acquired in the pandas Foundations course, Module One. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEau5nEvJpgm"
   },
   "source": [
    "#### 1.1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Bt_Q_oPJpgn"
   },
   "outputs": [],
   "source": [
    "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dataframe_image as dfi\n",
    "\n",
    "# Load the pyplot collection of functions from matplotlib, as plt \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "koUrawxsJpgq"
   },
   "source": [
    "#### 1.2.  Loading the data\n",
    "Your data comes from the [London Datastore](https://data.london.gov.uk/): a free, open-source data-sharing portal for London-oriented datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AiLiD4v3Jpgr"
   },
   "outputs": [],
   "source": [
    "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
    "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
    "\n",
    "url_LondonHousePrices = 'https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls'\n",
    "\n",
    "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
    "# As a result, we need to specify the sheet name in the read_excel() method.\n",
    "# Put this data into a variable called properties.  \n",
    "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "POukEJXgJpgu"
   },
   "source": [
    "### 2. Cleaning, transforming, and visualizing\n",
    "This second stage is arguably the most important part of any Data Science project. The first thing to do is take a proper look at the data. Cleaning forms the majority of this stage, and can be done both before or after Transformation.\n",
    "\n",
    "The end goal of data cleaning is to have tidy data. When data is tidy: \n",
    "\n",
    "1. Each variable has a column.\n",
    "2. Each observation forms a row.\n",
    "\n",
    "Keep the end goal in mind as you move through this process, every step will take you closer. \n",
    "\n",
    "\n",
    "\n",
    "***Hint:*** This part of the data science pipeline should test those skills you acquired in: \n",
    "- Intermediate Python for data science, all modules.\n",
    "- pandas Foundations, all modules. \n",
    "- Manipulating DataFrames with pandas, all modules.\n",
    "- Data Types for Data Science, Module Four.\n",
    "- Python Data Science Toolbox - Part One, all modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Te0Q548tnzZa"
   },
   "source": [
    "**2.1. Exploring your data** \n",
    "\n",
    "Think about your pandas functions for checking out a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rxirxw_qoAJa"
   },
   "outputs": [],
   "source": [
    "print('Dataframe head:\\n', properties.head(), '\\n')\n",
    "print('Dataframe shape:\\n', properties.shape, '\\n')\n",
    "print('Dataframe indices:\\n', properties.index, '\\n')\n",
    "print('Dataframe columns:\\n', properties.columns, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tE9Sqt9-oAta"
   },
   "source": [
    "**2.2. Cleaning the data**\n",
    "\n",
    "You might find you need to transpose your dataframe, check out what its row indexes are, and reset the index. You  also might find you need to assign the values of the first row to your column headings  . (Hint: recall the .columns feature of DataFrames, as well as the iloc[] method).\n",
    "\n",
    "Don't be afraid to use StackOverflow for help  with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdAu1A3YoH_r"
   },
   "outputs": [],
   "source": [
    "#Transposing dataframe and then printing it.\n",
    "properties_t = properties.transpose()\n",
    "print('Transpose of dataframe:\\n', properties_t.head(), '\\n')\n",
    "\n",
    "#Setting the first row as column headers and then dropping it.\n",
    "properties_t.columns = properties_t.iloc[0]\n",
    "properties_cleaned = properties_t.drop('Unnamed: 0').reset_index()\n",
    "\n",
    "print('Dataframe after cleaning:')\n",
    "properties_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1uLbJAsoIjK"
   },
   "source": [
    "**2.3. Cleaning the data (part 2)**\n",
    "\n",
    "You might we have to **rename** a couple columns. How do you do this? The clue's pretty bold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKkmn1AnoVZS"
   },
   "outputs": [],
   "source": [
    "#Renaiming columns containing Londong Boroughs and ID's\n",
    "properties_renamed = properties_cleaned.rename(columns={'index':'london_borough', pd.NaT:'borough_id'})\n",
    "\n",
    "#Checking indices and columns of new dataframe.\n",
    "print(properties_renamed.index)\n",
    "print(properties_renamed.columns)\n",
    "print('\\n')\n",
    "\n",
    "#Printing renamed dataframe.\n",
    "print('Dataframe after renaming:')\n",
    "properties_renamed.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jy8BzXHmoWEw"
   },
   "source": [
    "**2.4.Transforming the data**\n",
    "\n",
    "Remember what Wes McKinney said about tidy data? \n",
    "\n",
    "You might need to **melt** your DataFrame here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2wM0qLuo2Zt"
   },
   "outputs": [],
   "source": [
    "#Melting the dataframe, then printing it.\n",
    "properties_melted = properties_renamed.melt(id_vars=['london_borough', 'borough_id'], var_name='month', \\\n",
    "                                          value_name='ave_housing_price')\n",
    "print('Dataframe after melting: ')\n",
    "properties_melted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7kIsgAo7o3mf"
   },
   "source": [
    "Remember to make sure your column data types are all correct. Average prices, for example, should be floating point numbers... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZcR4IHbcpOaq"
   },
   "outputs": [],
   "source": [
    "#Checking the type of all column entries.\n",
    "for column in properties_melted.columns:\n",
    "    print('Type of ', column, ': ', properties_melted[column].dtype)\n",
    "print('\\n')\n",
    "\n",
    "#Applying float() function to entries of ave_housing_price column.\n",
    "#Make a new copy of dataframe with updated values.\n",
    "properties_melted_v2 = properties_melted\n",
    "properties_melted_v2['ave_housing_price'] = properties_melted['ave_housing_price'].apply(float)\n",
    "\n",
    "#Checking type of ave_housing_price column after applying float().\n",
    "print('ave_housing_price after applying float():')\n",
    "properties_melted_v2['ave_housing_price'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "knLUXHLypOtw"
   },
   "source": [
    "**2.5. Cleaning the data (part 3)**\n",
    "\n",
    "Do we have an equal number of observations in the ID, Average Price, Month, and London Borough columns? Remember that there are only 32 London Boroughs. How many entries do you have in that column? \n",
    "\n",
    "Check out the contents of the London Borough column, and if you find null values, get rid of them however you see fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BnvTW5a3p0fC"
   },
   "outputs": [],
   "source": [
    "#For every column in the dataframe, check how many NaN values there are.\n",
    "print('Number of NaN values in each column:\\n', properties_melted_v2[:].isna().sum(), '\\n')\n",
    "\n",
    "#Make filter to get rid of rows with NaN values.\n",
    "#In this list, \"True\" means it will be kept in the list\n",
    "NaN_filter = properties_melted_v2['borough_id'].isna() == False\n",
    "\n",
    "#Applying filter\n",
    "properties_no_nan = properties_melted_v2.loc[NaN_filter]\n",
    "\n",
    "#Checking how many rows where eliminated by subtracting the number of \n",
    "#indices in new dataframe from the old dataframe\n",
    "print('Number of rows eliminated: ', properties_melted_v2.shape[0] - properties_no_nan.shape[0], '\\n')\n",
    "\n",
    "#Another check to see if there are any NaN values remaining in the new dataframe.\n",
    "print('Number of NaN values in each column after filtering:\\n', \\\n",
    "      properties_no_nan[:].isna().sum(), '\\n')\n",
    "\n",
    "#Checking how many unique values are in the 'london_boroughs' column. There should be 32. However...\n",
    "print('Unique values in the \"boroughs\" column: ', properties_no_nan['london_borough'].nunique(), '\\n')\n",
    "\n",
    "#One way to distinguish wheter a string in the london_borough columns is or is not a borough\n",
    "#is to manually look at is. Luckily, it seems our source data file already had the boroughs\n",
    "#sorted at the top, as can be seen from the following list.\n",
    "print('List of unique string in the london_borough column:')\n",
    "for unique_borough in properties_no_nan['london_borough'].unique():\n",
    "    print(unique_borough)\n",
    "print('Note how only the first 32 are actually boroughs.\\n')\n",
    "\n",
    "#Makingn an array of strings containing only the boroughs by slicing the previous list.\n",
    "#Then, checking that it has 32 items\n",
    "boroughs_array = properties_no_nan['london_borough'].unique()[:32]\n",
    "print('Number of boroughs in boroughs_array: ', len(boroughs_array))\n",
    "\n",
    "#Finally, we use the array of boroughs to filter our previous dataframe\n",
    "properties_boroughs_only \\\n",
    "= properties_no_nan.loc[properties_no_nan['london_borough'].isin(boroughs_array)].reset_index(drop=True)\n",
    "\n",
    "#As a final check, counting the total number of unique strings in the london_borough column of our updated\n",
    "#dataframe\n",
    "print('Unique strings in the london_borough column of the updated dataframe: ', \\\n",
    "      properties_boroughs_only['london_borough'].nunique())\n",
    "\n",
    "#Print our most recent dataframe.\n",
    "print('Dataframe with only London Boroughs included:\\n')\n",
    "properties_boroughs_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGEx6mJsp6dG"
   },
   "source": [
    "**2.6. Visualizing the data**\n",
    "\n",
    "To visualize the data, why not subset on a particular London Borough? Maybe do a line plot of Month against Average Price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAg5pT9cqHAR"
   },
   "outputs": [],
   "source": [
    "#The previously defined boroughs_array will be used to make ave_housing_price vs month plots for each\n",
    "#London Borough\n",
    "\n",
    "#Defining function that uses the first n entries in an array 'array' of borough names to plot \n",
    "#average housing vs time from a dataframe df\n",
    "def plot_n_boroughs(df, array, n):\n",
    "    for i in range(n):\n",
    "        filtered_df = df.loc[df['london_borough'] == array[i]]\n",
    "        plt.plot(filtered_df['month'], filtered_df['ave_housing_price'], label=array[i])\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Average Housing Price')\n",
    "    plt.title('Average Housing Price vs. Time for various London Boroughs')\n",
    "    plt.legend(title='Group')\n",
    "    \n",
    "#To get a sense for how the data looks like, 5 arbitrary boroughs will be plotted.\n",
    "plot_n_boroughs(properties_boroughs_only, boroughs_array, 5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aWTPqSJeqHnC"
   },
   "source": [
    "To limit the number of data points you have, you might want to extract the year from every month value your *Month* column. \n",
    "\n",
    "To this end, you *could* apply a ***lambda function***. Your logic could work as follows:\n",
    "1. look through the `Month` column\n",
    "2. extract the year from each individual value in that column \n",
    "3. store that corresponding year as separate column. \n",
    "\n",
    "Whether you go ahead with this is up to you. Just so long as you answer our initial brief: which boroughs of London have seen the greatest house price increase, on average, over the past two decades? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0DF92cyqnu8"
   },
   "outputs": [],
   "source": [
    "#Make copy of most recent dataframe and add a year column\n",
    "properties_final = properties_boroughs_only\n",
    "properties_final['year'] = properties_boroughs_only['month'].apply(lambda x: x.year)\n",
    "\n",
    "print(properties_final['ave_housing_price'])\n",
    "\n",
    "#What follows are some consistency checks.\n",
    "\n",
    "#Checking how many times each year appears. Since there are 12 months for each of the 32 boroughs, these\n",
    "#numbers should be 384 for each year, with the exception of the last year, which only has 3 months.\n",
    "print('Number of times each year appears:\\n', properties_final['year'].value_counts(), '\\n')\n",
    "\n",
    "#Checking total month entries for each London Borough. Since the years go from 1995 till 2022 and 2022\n",
    "#only has 3 months, this number should be 327. Also print out the total number of London Boroughs.\n",
    "\n",
    "print('Number of month entries per London Borough:')\n",
    "count = 0\n",
    "for group_tuple in list(properties_final.groupby('london_borough')):\n",
    "    count += 1\n",
    "    print(group_tuple[0], ': ', len(group_tuple[1]['month']))\n",
    "print('Number of Boroughs: ', count, '\\n')\n",
    "\n",
    "#Change float format of ave_housing_price column:\n",
    "#properties_final['ave_housing_price'] = properties_final['ave_housing_price'].apply(lambda x: '%.2f' % x)\n",
    "print(properties_final['ave_housing_price'])\n",
    "\n",
    "#Print out the final dataframe.\n",
    "print('Final dataframe:')\n",
    "properties_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2knuTxAEqoJ4"
   },
   "source": [
    "**3. Modeling**\n",
    "\n",
    "Consider creating a function that will calculate a ratio of house prices, comparing the price of a house in 2018 to the price in 1998.\n",
    "\n",
    "Consider calling this function create_price_ratio.\n",
    "\n",
    "You'd want this function to:\n",
    "1. Take a filter of dfg, specifically where this filter constrains the London_Borough, as an argument. For example, one admissible argument should be: dfg[dfg['London_Borough']=='Camden'].\n",
    "2. Get the Average Price for that Borough, for the years 1998 and 2018.\n",
    "4. Calculate the ratio of the Average Price for 1998 divided by the Average Price for 2018.\n",
    "5. Return that ratio.\n",
    "\n",
    "Once you've written this function, you ultimately want to use it to iterate through all the unique London_Boroughs and work out the ratio capturing the difference of house prices between 1998 and 2018.\n",
    "\n",
    "Bear in mind: you don't have to write a function like this if you don't want to. If you can solve the brief otherwise, then great! \n",
    "\n",
    "***Hint***: This section should test the skills you acquired in:\n",
    "- Python Data Science Toolbox - Part One, all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKTyr437UgDa"
   },
   "outputs": [],
   "source": [
    "#Function definition\n",
    "def create_price_ratio(borough_arg):\n",
    "    filtered_table = properties_final[properties_final['london_borough'] == borough_arg]\n",
    "    p_table = filtered_table.pivot_table(index='year', values='ave_housing_price') #Calculates average by default\n",
    "    return p_table.at[1998, 'ave_housing_price'] / p_table.at[2018, 'ave_housing_price']\n",
    "\n",
    "#Create dictionary containing each London Borough and its associated price ratio as a key-value pair.\n",
    "ratio_dict = {}\n",
    "for borough in properties_final['london_borough'].unique():\n",
    "    ratio = create_price_ratio(borough)\n",
    "    ratio_dict[borough] = ratio\n",
    "    \n",
    "#Sort the dictionary in ascending order of ratio.\n",
    "ratio_dict_sorted = dict(sorted(ratio_dict.items(), key=lambda x: x[1]))\n",
    "\n",
    "#Making a pandas dataframe from this dictionary\n",
    "ratios_df = pd.DataFrame(list(ratio_dict_sorted.items()))\n",
    "ratios_df_split = pd.concat([ratios_df.iloc[:16], ratios_df.iloc[16:].reset_index(drop=True)], axis=1)\n",
    "ratios_df_split.rename(columns={0:'London Borough', 1:'Ratio'}, inplace=True)\n",
    "blankIndex = [''] * len(ratios_df_split)\n",
    "ratios_df_split.index = blankIndex\n",
    "\n",
    "#Printing this dataframe.\n",
    "print('Price ratio of each London borough:\\n', ratios_df_split)\n",
    "\n",
    "#Exporting this dataframe.\n",
    "ratios_df_split.dfi.export('ratios_table.png')\n",
    "\n",
    "\n",
    "#Extract the key of the first item on the dictionary:\n",
    "answer = list(ratio_dict_sorted.keys())[0]\n",
    "\n",
    "#Print out the final answer.\n",
    "print('The London Borough that has shown the greatest increase in average house pricing \\\n",
    "from 1998 to 2918 is:\\n', answer, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These final lines of code make Average Hosing Price vs. Time plots of 5 arbitrary boroughs plus Kackney. \n",
    "#A quick glance of it tells us that, while Hackney had the fastest price increase in a ten-year period, \n",
    "#it is also not the borough with the most expensive housing.\n",
    "print('The purpose of this plot is to show that, while Hackney had the largest increase, it does not have \\\n",
    "the most\\n expensive housing.')\n",
    "\n",
    "df_hackney = properties_final[properties_final['london_borough'] == 'Hackney'] \n",
    "plt.plot(df_hackney['month'], df_hackney['ave_housing_price'], label='Hackney')\n",
    "plot_n_boroughs(properties_final, boroughs_array, 5)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "#Bar plot of the 5 London boroughs that showed the greates increase in average housing prie.\n",
    "ratios_df.iloc[:5].plot(y=1, kind='bar', rot=45, \\\n",
    "                        legend=False, title='Ratio or average housing price of 1998 to 2018', \\\n",
    "                       ylabel='ratio', x=0, xlabel='')\n",
    "plt.savefig('ratios_bar.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "#Average housing price vs time plot of these 5 boroughs.\n",
    "array_sorted_by_ratio = np.array(ratios_df[0])\n",
    "plot_n_boroughs(properties_final, array_sorted_by_ratio, 5)\n",
    "plt.savefig('line_plot_top_ratios', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe containing the max housing price that each borough ever had.\n",
    "max_price_df = properties_final.pivot_table(values='ave_housing_price', index='london_borough', aggfunc='max')\n",
    "#print(max_price_df)\n",
    "#print(max_price_df.columns)\n",
    "max_price_df.rename(columns={'ave_housing_price':'max_housing_price'}, inplace=True)\n",
    "max_price_df.sort_values('max_housing_price', ascending=False, inplace=True)\n",
    "max_price_df.reset_index(inplace=True)\n",
    "max_price_df['max_housing_price'] = max_price_df['max_housing_price'].apply(lambda x: '%.2f' % x)\n",
    "print(max_price_df)\n",
    "#print(max_price_df.columns)\n",
    "#print(max_price_df.iloc[:16], '\\n\\n', max_price_df.iloc[16:].reset_index(drop=True))\n",
    "max_price_df_split = pd.concat([max_price_df.iloc[:16], max_price_df.iloc[16:].reset_index(drop=True)], axis=1)\n",
    "#print(max_price_df_split)\n",
    "\n",
    "#max_price_df_split.rename(columns={0:'London Borough', 1:'Max Housing Price'}, inplace=True)\n",
    "blankIndex = [''] * len(max_price_df_split)\n",
    "max_price_df_split.index = blankIndex\n",
    "max_price_df_split.dfi.export('max_price.png')\n",
    "print(max_price_df_split)\n",
    "\n",
    "#Creating numpy array containing the boroughs sorted by max housing price.\n",
    "array_sorted_by_max_price = np.array(max_price_df.iloc[:, 0])\n",
    "plot_n_boroughs(properties_final, array_sorted_by_max_price, 5)\n",
    "plt.savefig('line_plot_top_max_price', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzYUI7FxJpgv"
   },
   "source": [
    "### 4. Conclusion\n",
    "What can you conclude? Type out your conclusion below. \n",
    "\n",
    "Look back at your notebook. Think about how you might summarize what you have done, and prepare a quick presentation on it to your mentor at your next meeting. \n",
    "\n",
    "We hope you enjoyed this practical project. It should have consolidated your data hygiene and pandas skills by looking at a real-world problem involving just the kind of dataset you might encounter as a budding data scientist. Congratulations, and looking forward to seeing you at the next step in the course! Look back at your notebook. Think about how you might summarize what you have done, and prepare a quick presentation on it to your mentor at your next meeting.\n",
    "\n",
    "Conclusions:\n",
    "The preceding analysis shows that, from the year 1998 to 2018, the London borough that had the greatest average housing price increase was Hackney. A quick glance at the average housing price ratios from the year 1998 to 2018 for all boroughs show that many other boroughs have similar ratios, so Hackley is by no means an outlier.\n",
    "\n",
    "Another interesting obsevation is that every borough has a sharp decrease in housing price at about the year 2009. Based on the timing, my current hypothesis is that it may be related to the United States housing bubble that occured at around that time, but more analysis is needed to draw definite conclusions.\n",
    "\n",
    "Yet another interesting observations is that certain boroughs show more variability in their housing prices than others, City of London being a noteworthy example. Perhaps this could also be used to somehow quantify the \"popularity\" for living in each borough.\n",
    "\n",
    "Finally, it should be noted that, while Hackney showed the greatest price increase in hosusing prices, it is by no means the borough with the most expensive housing."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Springboard Data Science Career Track Unit 4 Challenge - Tier 3 Complete .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
