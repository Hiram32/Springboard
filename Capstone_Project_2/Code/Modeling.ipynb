{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Training](#Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, RocCurveDisplay, roc_auc_score, roc_curve, ConfusionMatrixDisplay, \\\n",
    "    precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Data/df_clean_null.pkl'\n",
    "df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['_RFHLTH', '_HCVU651', '_RFHYPE5', '_RFCHOL', '_ASTHMS1', '_DRDXAR1', '_RACEGR3', '_AGE_G', '_BMI5CAT', '_EDUCAG', '_INCOMG', '_SMOKER3',\\\n",
    "    '_RFDRHV5', '_PACAT1', '_RFSEAT2', '_FLSHOT6', '_PNEUMO2', '_AIDTST3', 'CHCCOPD1', 'ADDEPEV2', 'CHCKIDNY', 'DIABETE3', 'SEX', 'MARITAL', 'DRADVISE']\n",
    "target_name = ['_MICHD']\n",
    "weights_name = ['_LLCPWT'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[feature_names + target_name + weights_name]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['_RFHYPE5', '_RFCHOL', '_RFDRHV5']:\n",
    "    df[column] = df[column].apply(lambda x: 1.0 if x == 2.0 else 2.0 if x == 1.0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['_FLSHOT6', '_PNEUMO2']:\n",
    "    df[column] = df[column].cat.add_categories([65.0])\n",
    "    df[column] = df[column].fillna(65.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DIABETE3'] = df['DIABETE3'].map({1.0:1.0, 2.0:1.0, 3.0:2.0, 4.0:2.0, 7.0:7.0, 9.0:9.0}).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['_MICHD'] = df['_MICHD'].apply(lambda x: 0.0 if x == 2 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputer(dataframe, category_value_tofill=None, columns_drop=None, columns_mode=None, columns_median=None):\n",
    "    '''Imputes missing values to the input dataframe.\n",
    "    \n",
    "       Parameters\n",
    "       ----------\n",
    "       dataframe: Pandas dataframe\n",
    "           dataframe with which to impute missing values.\n",
    "       \n",
    "       category_value_tofill: int, float, or string\n",
    "           Value to used to fill missing values in categorical features.\n",
    "       \n",
    "       columns_drop: list-like\n",
    "           List of columns to drop.\n",
    "       \n",
    "       columns_mode: list-like\n",
    "           List of numeric columns to impute with the mode.\n",
    "           \n",
    "       columns_median: list-like\n",
    "           List of numeric columns to impute with the mean.\n",
    "    '''\n",
    "    #Fill null values in categorical features with value_null\n",
    "    if category_value_tofill != None:\n",
    "        for column in dataframe.select_dtypes(include='category').columns:\n",
    "            if any(dataframe[column].isnull()):\n",
    "                dataframe[column] = dataframe[column].cat.add_categories([category_value_tofill])\n",
    "                dataframe[column] = dataframe[column].fillna(value=category_value_tofill)\n",
    "            \n",
    "    #Droping columns, imputing with mode, and imputing with median.\n",
    "    if columns_drop != None:\n",
    "        dataframe = dataframe.drop(columns=columns_drop)\n",
    "    if columns_mode != None:\n",
    "        dataframe = dataframe.fillna(dataframe[columns_mode].mode().iloc[0, :])\n",
    "    if columns_median != None:\n",
    "        dataframe = dataframe.fillna(dataframe[columns_median].median())\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = imputer(df, 999.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list = ['Good_Health', 'Health_Insurance', 'Hypertension', 'High_Cholesterol', 'Asthma_Status', 'Arthritis', 'Race', 'Age_Cat', 'BMI_Cat', 'Education_Level', 'Income_Level', \\\n",
    "    'Smoker_Status', 'Heavy_Drinker', 'Physical_Activity', 'Seatbelt', 'Flu_Shot', 'Pneumonia_Vaccine', 'HIV', 'Bronchitis', 'Depression', 'Kidney_Disease', 'Diabetes', 'SEX', \\\n",
    "        'Marital Status', 'Sodium', 'Heart_Disease', 'Sample_Weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_dict = dict(zip(feature_names + target_name + weights_name, trans_list))\n",
    "trans_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns=trans_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df[trans_dict[target_name[0]]] == 999.0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_one_hot = [column for column in df.select_dtypes(include='category').columns if column not in ['Heart_Disease']]\n",
    "\n",
    "column_trans = ColumnTransformer([('categorical', OneHotEncoder(sparse=False), cols_to_one_hot)], remainder='passthrough')\n",
    "column_trans.fit(df)\n",
    "column_names_trans = np.concatenate([column_trans.named_transformers_['categorical'].get_feature_names_out(), np.array([trans_dict[target_name[0]], trans_dict[weights_name[0]]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(column_trans.transform(df), columns=column_names_trans)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/modeling_df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "    pickle.dump(trans_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/modeling_df.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "    \n",
    "y = df['Heart_Disease']\n",
    "w = df['Sample_Weights']\n",
    "X = df.drop(columns=[y.name, w.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionCustom(LogisticRegression):\n",
    "    def __init__(self, max_iter=100, class_weight=None, threshold=None):\n",
    "        super().__init__(max_iter=max_iter, class_weight=class_weight)\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.threshold != None:\n",
    "            return (super().predict_proba(X)[:, 1] >= self.threshold).astype(int)\n",
    "        else:\n",
    "            return super().predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logistic_feature_importance(coefficients, feature_names, number):\n",
    "    coef_enum = enumerate(coefficients)\n",
    "    coef_enum_sorted_top = sorted(coef_enum, key=lambda x: abs(x[1]), reverse=True)[:number]\n",
    "    \n",
    "    indices = list(list(zip(*coef_enum_sorted_top))[0])\n",
    "    names = feature_names[indices]\n",
    "    top_coef = np.array(list(zip(*coef_enum_sorted_top))[1])\n",
    "    \n",
    "    neg_pos = np.where(top_coef < 0)[0]\n",
    "    pos_pos = np.where(top_coef >= 0)[0]\n",
    "    neg_coef = top_coef[neg_pos]\n",
    "    pos_coef = top_coef[pos_pos]\n",
    "    neg_names = names[neg_pos]\n",
    "    pos_names = names[pos_pos]\n",
    "    \n",
    "    _, ax = plt.subplots()\n",
    "    \n",
    "    ax.bar(neg_pos, np.abs(neg_coef), color='r', label='Negative')\n",
    "    ax.bar(pos_pos, np.abs(pos_coef), color='b', label='Positive')\n",
    "    ax.set_xticks(range(0, len(names)))\n",
    "    ax.set_xticklabels(names, rotation=90)\n",
    "    ax.set_title('Top Features by Absolute Value of Model Coefficient')\n",
    "    ax.set_ylabel('Feature Coefficient')\n",
    "    ax.set_xlabel('Features')\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(y_true, y_proba, set_tpr, sample_weight=None):\n",
    "    _, tpr_list, threshold_list = roc_curve(y_true, y_proba, sample_weight=sample_weight)\n",
    "    for tpr, threshold in zip(tpr_list, threshold_list):\n",
    "        if (tpr >= set_tpr):\n",
    "            return tpr, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_scorer(y_test, y_pred, y_pred_proba=None, sample_weight=None, model_name=None, output=True):\n",
    "    prfs_scores = precision_recall_fscore_support(y_test, y_pred, sample_weight=sample_weight)\n",
    "    roc_auc_score_ = roc_auc_score(y_test, y_pred, sample_weight=sample_weight)\n",
    "    scores = *prfs_scores, roc_auc_score_\n",
    "    \n",
    "    cf_display = ConfusionMatrixDisplay.from_predictions(y_test, y_pred, sample_weight=sample_weight, normalize = 'true')\n",
    "    if output == True:\n",
    "        if type(model_name) != type(None):\n",
    "            plt.title(model_name)\n",
    "        plt.show()\n",
    "        print()\n",
    "        \n",
    "    if output == True:\n",
    "        print(f'Classification Report: {model_name if type(model_name) != type(None) else \"\"}')\n",
    "    class_report = classification_report(y_test, y_pred, sample_weight=sample_weight)\n",
    "    if output == True:\n",
    "        print(class_report)\n",
    "        print()\n",
    "    \n",
    "    if type(y_pred_proba) != type(None):\n",
    "        roc_display = RocCurveDisplay.from_predictions(y_test, y_pred_proba, sample_weight=sample_weight)\n",
    "        if output == True:\n",
    "            if model_name != None:\n",
    "                plt.title(model_name)\n",
    "            plt.show()\n",
    "    else:\n",
    "        roc_display = None\n",
    "        \n",
    "    return scores, (cf_display, class_report, roc_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pipeline(ml_algo, *data, model_name=None, algo_params={}, resampler=None, split_params={}, output=True):\n",
    "    if len(data) not in [2, 3]:\n",
    "        print(f'Invalid length for \"data\": {len(data)}.')\n",
    "        return\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    split_result = train_test_split(*data, **split_params)\n",
    "    if len(split_result) == 4:\n",
    "        X_train, X_test, y_train, y_test = split_result\n",
    "        w_train = None\n",
    "        w_test = None\n",
    "    elif len(split_result) == 6:\n",
    "        X_train, X_test, y_train, y_test, w_train, w_test = split_result\n",
    "    else:\n",
    "        print('Invalid length for \"split_result\".')\n",
    "        return\n",
    "        \n",
    "    if type(resampler) != type(None):\n",
    "        X_train, y_train = resampler().fit_resample(X_train, y_train)\n",
    "    \n",
    "    model = ml_algo(**algo_params)\n",
    "    model.fit(X_train, y_train, sample_weight=w_train)\n",
    "    results['model_info'] = model_name, model\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba_train = None\n",
    "        y_pred_proba = None\n",
    "        \n",
    "    results['predictions_train'] = y_pred_train, y_pred_proba_train\n",
    "    results['predictions'] =y_pred, y_pred_proba\n",
    "    \n",
    "    results['scores_train'], results['displays_train'] = my_scorer(y_train, y_pred_train, y_pred_proba_train, sample_weight=w_train, model_name=model_name + ' (train)', output=output)\n",
    "    results['scores'], results['displays'] = my_scorer(y_test, y_pred, y_pred_proba, sample_weight=w_test, model_name=model_name, output=output)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_cv_scorer(ml_algo, scoring, *data, model_name=None, param_grid={}, resampler=None, output=True):    \n",
    "    if len(data) == 2:\n",
    "        X, y = tuple(data)\n",
    "        w = None\n",
    "    elif len(data) == 3:\n",
    "        X, y, w = tuple(data)\n",
    "    else:\n",
    "        print('Invalid length for \"data\".')\n",
    "        return\n",
    "    \n",
    "    if type(resampler) != type(None):\n",
    "        model = Pipeline([('Resampling', resampler()), (model_name, ml_algo())])\n",
    "    else:\n",
    "        model = Pipeline([(model_name, ml_algo())])\n",
    "        \n",
    "    keys = [k for k in param_grid.keys()]\n",
    "    for key in keys:\n",
    "        param_grid[model_name + '__' + key] = param_grid.pop(key)\n",
    "    \n",
    "    clf = GridSearchCV(model, param_grid=param_grid, scoring=scoring, refit=False)\n",
    "    clf.fit(X, y, CVSearch_Test__sample_weight=w)\n",
    "    cv_scores = clf.cv_results_\n",
    "    print('Printing cv_scores keys:')\n",
    "    print(cv_scores.keys())\n",
    "    print('Printing cv_scores:')\n",
    "    print(cv_scores)\n",
    "    if output == True:\n",
    "        if type(model_name) != type(None):\n",
    "            print(model_name + ' cv_scores:')\n",
    "        else:\n",
    "            print('cv_scores:')\n",
    "        print(cv_scores)\n",
    "        print()\n",
    "    \n",
    "    dict_scores = {}\n",
    "    #dict_scores['estimator'] = cv_scores['estimator']\n",
    "    for score in scoring:\n",
    "        #scores_ = cv_scores['test_' + score]\n",
    "        mean_array = cv_scores['mean_test_' + score]\n",
    "        std_array = cv_scores['std_test_' + score]\n",
    "        \n",
    "        dict_scores[score]= dict(zip(['mean', 'std'], [mean_array, std_array]))\n",
    "        \n",
    "        if output == True:\n",
    "            print(score + ' mean: ', mean_array)\n",
    "            print(score + ' std: ', std_array)\n",
    "            print()\n",
    "    if print == True:\n",
    "        print()\n",
    "    \n",
    "    if type(model_name) != type(None):\n",
    "        return {model_name:dict_scores}\n",
    "    else:\n",
    "        return dict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_scorer(ml_algo, scoring, *data, model_name=None, algo_params={}, resampler=None, output=True):    \n",
    "    if len(data) == 2:\n",
    "        X, y = tuple(data)\n",
    "        w = None\n",
    "    elif len(data) == 3:\n",
    "        X, y, w = tuple(data)\n",
    "    else:\n",
    "        print('Invalid length for \"data\".')\n",
    "        return\n",
    "    \n",
    "    if type(resampler) != type(None):\n",
    "        model = Pipeline([('Resampling', resampler()), (model_name, ml_algo(**algo_params))])\n",
    "    else:\n",
    "        model = Pipeline([(model_name, ml_algo(**algo_params))])\n",
    "    \n",
    "    cv_scores = cross_validate(model, X, y, scoring=scoring, fit_params={model_name + '__sample_weight':w}, return_estimator=True)\n",
    "    if output == True:\n",
    "        if type(model_name) != type(None):\n",
    "            print(model_name + ' cv_scores:')\n",
    "        else:\n",
    "            print('cv_scores:')\n",
    "        print(cv_scores)\n",
    "        print()\n",
    "    \n",
    "    dict_scores = {}\n",
    "    dict_scores['estimator'] = cv_scores['estimator']\n",
    "    for score in scoring:\n",
    "        scores_ = cv_scores['test_' + score]\n",
    "        mean_ = cv_scores['test_' + score].mean()\n",
    "        std_ = cv_scores['test_' + score].std()\n",
    "        \n",
    "        dict_scores[score]= dict(zip(['scores', 'mean', 'std'], [scores_, mean_, std_]))\n",
    "        \n",
    "        if output == True:\n",
    "            print(score + ' mean: ' + f'{mean_:0.2f}')\n",
    "            print(score + ' std: ' + f'{std_:0.4f}')\n",
    "            print()\n",
    "    if print == True:\n",
    "        print()\n",
    "    \n",
    "    if type(model_name) != type(None):\n",
    "        return {model_name:dict_scores}\n",
    "    else:\n",
    "        return dict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(importances, labels, title=None, num=20):\n",
    "    num = num * (-1) - 1\n",
    "    indices = importances.argsort()[:num:-1]\n",
    "    plt.bar(x=labels[indices], height=importances[indices])\n",
    "    plt.xticks(rotation=90)\n",
    "    if type(title) != type(None):\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score_pos = make_scorer(precision_score, pos_label=1)\n",
    "precision_score_neg = make_scorer(precision_score, pos_label=0)\n",
    "\n",
    "recall_score_pos = make_scorer(recall_score, pos_label=1)\n",
    "recall_score_neg = make_scorer(recall_score, pos_label=0)\n",
    "\n",
    "f1_score_pos = make_scorer(f1_score, pos_label=1)\n",
    "f1_score_neg = make_scorer(f1_score, pos_label=0)\n",
    "\n",
    "roc_auc_scorer = make_scorer(roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv_results = grid_cv_scorer(LogisticRegressionCustom, {'precision':precision_score_pos, 'recall':recall_score_pos}, X, y, model_name='CVSearch_Test', param_grid={'max_iter':[500, 600]}, \\\n",
    "    resampler=RandomOverSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_list = {'Logistic Regression':LogisticRegressionCustom, 'Random Forest':RandomForestClassifier}\n",
    "algo_params_list = [dict(max_iter=500), dict(n_estimators=100)]\n",
    "imbalance_strategy_list = {'Basic':None, 'Class Weight':None, 'Random Oversmpling':RandomOverSampler, 'Smote':SMOTE, 'Random Undersampler':RandomUnderSampler}\n",
    "scores = {'precision_pos':precision_score_pos, 'precision_neg':precision_score_neg, 'recall_pos':recall_score_pos, 'recall_neg':recall_score_neg, \\\n",
    "    'f1_pos':f1_score_pos, 'f1_neg':f1_score_neg, 'roc_auc':roc_auc_scorer}\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for algo_name, algo_params in zip(algo_list, algo_params_list):\n",
    "    for resampler_name, resampler in imbalance_strategy_list.items():\n",
    "        if resampler_name == 'Basic':\n",
    "            data = X, y, w\n",
    "        else:\n",
    "            data = X, y\n",
    "        if resampler_name == \"Class Weight\":\n",
    "            algo_params['class_weight'] = 'balanced'\n",
    "            \n",
    "        cv_results.update(cv_scorer(algo_list[algo_name], scores, *data, model_name=algo_name+', '+resampler_name, algo_params=algo_params, resampler=resampler))\n",
    "        \n",
    "        with open('../Data/modeling_results.pkl', 'wb') as f:\n",
    "            pickle.dump(cv_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_list = {'Random Forest':RandomForestClassifier}\n",
    "algo_params_list = [dict(n_estimators=100)]\n",
    "imbalance_strategy_list = {'Class Weight':None, 'Random Oversmpling':RandomOverSampler, 'Smote':SMOTE, 'Random Undersampler':RandomUnderSampler}\n",
    "scores = {'precision_pos':precision_score_pos, 'precision_neg':precision_score_neg, 'recall_pos':recall_score_pos, 'recall_neg':recall_score_neg, \\\n",
    "    'f1_pos':f1_score_pos, 'f1_neg':f1_score_neg, 'roc_auc':roc_auc_scorer}\n",
    "\n",
    "for algo_name, algo_params in zip(algo_list, algo_params_list):\n",
    "    for resampler_name, resampler in imbalance_strategy_list.items():\n",
    "        if resampler_name == 'Basic':\n",
    "            data = X, y, w\n",
    "        else:\n",
    "            data = X, y\n",
    "        if resampler_name == \"Class Weight\":\n",
    "            algo_params['class_weight'] = 'balanced'\n",
    "            \n",
    "        cv_results.update(cv_scorer(algo_list[algo_name], scores, *data, model_name=algo_name+', '+resampler_name, algo_params=algo_params, resampler=resampler))\n",
    "        \n",
    "        with open('../Data/modeling_results.pkl', 'wb') as f:\n",
    "            pickle.dump(cv_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/modeling_results.pkl', 'rb') as f:\n",
    "    cv_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.pop('Random Forest, Basic')\n",
    "cv_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "for score in scores:\n",
    "    columns.append(score + ' mean')\n",
    "    columns.append(score + ' std')\n",
    "    \n",
    "data = []\n",
    "index = []\n",
    "for model_name in cv_results:\n",
    "    index.append(model_name)\n",
    "    row = []\n",
    "    for score in cv_results[model_name]:\n",
    "        row.append(cv_results[model_name][score]['mean'])\n",
    "        row.append(cv_results[model_name][score]['std'])\n",
    "    data.append(row)\n",
    "columns, index, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(data, index=index, columns=columns)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/modeling_results.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "88290e038b87e2cdb6a37058d27d18dea0a708f5e3ed04873cad7201dff76582"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
